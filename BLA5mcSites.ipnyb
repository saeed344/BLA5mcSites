{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from features import ensembleFeature\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport torch\nfrom torch import nn\nimport re\nfrom sklearn.utils import shuffle\nfrom utils import metricsCal\nfrom torch.utils.data import DataLoader, TensorDataset\nimport math\nimport sys\nimport copy\nimport pickle\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport os\nimport Attention_model as Model\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\nimport csv\n\n\ndef read_fasta(fasta_file_path):\n    \"\"\"\n    Read sequences and names from a FASTA file.\n\n    Parameters:\n    - fasta_file_path (str): Path to the FASTA file.\n\n    Returns:\n    - sequences (list): List of RNA sequences.\n    - names (list): List of names corresponding to RNA sequences.\n    \"\"\"\n    sequences = []\n    names = []\n\n    with open(fasta_file_path, 'r') as fasta_file:\n        for line in fasta_file:\n            if line.startswith('>'):\n                name = line.strip()[1:]\n                names.append(name)\n                sequence = ''\n            else:\n                sequence += line.strip()\n                sequences.append(sequence)\n    return sequences, names\n\n\n# Change the length of seq from 1001 to 41.\ndef long_short(data):\n    seq_list = []\n    for i in data[0]:\n        seq_list.append(str(i)[480:521])\n    return np.array(seq_list)\n\n\n# Delete the seq if its bases contain 'N', which is not 'ATCG'\ndef check_N(data1):\n    seq_list = []\n    for i in range(len(data1)):\n        if str(data1[i]).find(\"N\") < 0:\n            seq_list.append(data1[i])\n    return np.array(seq_list)\n\n\nif __name__ == '__main__':\n\n    device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n    device_cpu = torch.device(\"cpu\")\n\n    # Directly set the parameters\n    type_name = \"YourType\"  # Replace with your type name\n    cell_name = \"YourCell\"  # Replace with your cell name\n\n    # Hyper Parameters\n    max_epochs = 50\n    max_patience = 10\n    batch_size = 32\n    cuda_device = 0\n\n    # Choose the device\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_device)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    train_seq, name = read_fasta(\"data/\" + str(type_name) + \"+\" + str(cell_name) + \"/train.fa\")\n\n    trainPos_seq = []\n    trainNeg_seq = []\n    for i in range(len(name)):\n        if name[i][0] == 'P':\n            trainPos_seq.append(train_seq[i])\n        else:\n            trainNeg_seq.append(train_seq[i])\n\n    test_seq, name = read_fasta(\"data/\" + str(type_name) + \"+\" + str(cell_name) + \"/test.fa\")\n\n    testPos_seq = []\n    testNeg_seq = []\n    for i in range(len(name)):\n        if name[i][0] == 'P':\n            testPos_seq.append(test_seq[i])\n        else:\n            testNeg_seq.append(test_seq[i])\n\n    # Shuffle the seq\n    trainNeg_seq = shuffle(trainNeg_seq, random_state=1)\n    testNeg_seq = shuffle(testNeg_seq, random_state=1)\n\n    # Choose the seq\n    trainNeg_seq = trainNeg_seq[:len(trainPos_seq) * 10]\n    testNeg_seq = testNeg_seq[:len(testPos_seq)]\n\n    # Generate the feature\n    print(\"The model is generating the w2v feature from the seq now!\")\n    trainPos_emb = ensembleFeature.emb_seqs(trainPos_seq)\n    trainNeg_emb = ensembleFeature.emb_seqs(trainNeg_seq)\n\n    print(\"The model is generating the PSNP feature from the seq now!\")\n    trainPos_PSNP, trainNeg_PSNP, testPos_PSNP, testNeg_PSNP = ensembleFeature.PSNP(trainPos_seq, trainNeg_seq,\n                                                                                    testPos_seq, testNeg_seq)\n\n    print(\"The model is generating the PCP feature from the seq now, please waitâ€¦ this will take some time\")\n    trainPos_PCP = ensembleFeature.PCP(trainPos_seq)\n    trainNeg_PCP = ensembleFeature.PCP(trainNeg_seq)\n\n    print(\"The model is generating the DBPF feature from the seq now!\")\n    trainPos_DBPF = ensembleFeature.DBPF(trainPos_seq)\n    trainNeg_DBPF = ensembleFeature.DBPF(trainNeg_seq)\n\n    trainPos_PSNP = trainPos_PSNP.reshape(trainPos_PSNP.shape[0], 1, trainPos_PSNP.shape[1])\n    trainNeg_PSNP = trainNeg_PSNP.reshape(trainNeg_PSNP.shape[0], 1, trainNeg_PSNP.shape[1])\n\n    trainPos_PCP = trainPos_PCP.reshape(trainPos_PCP.shape[0], 1, trainPos_PCP.shape[1])\n    trainNeg_PCP = trainNeg_PCP.reshape(trainNeg_PCP.shape[0], 1, trainNeg_PCP.shape[1])\n\n    # Do the 5-fold cross validation\n    kf = KFold(5, shuffle=True)\n    for i, [train_index, test_index] in enumerate(kf.split(trainNeg_emb)):\n\n        # Generate the data for training and validation\n        X_train_PCP_neg = trainNeg_PCP[train_index]\n        X_test_PCP_neg = trainNeg_PCP[test_index]\n        trainPos_PCP = np.repeat(trainPos_PCP, 10, axis=0)\n        X_train_PCP_pos = trainPos_PCP[:int(0.8 * len(trainPos_PCP))]\n        X_test_PCP_pos = trainPos_PCP[int(0.8 * len(trainPos_PCP)):]\n        Y_train_PCP = np.append(np.ones(len(X_train_PCP_pos)), np.zeros(len(X_train_PCP_neg)), axis=0)\n        Y_test_PCP = np.append(np.ones(len(X_test_PCP_pos)), np.zeros(len(X_test_PCP_neg)), axis=0)\n        X_train_PCP = np.append(X_train_PCP_pos, X_train_PCP_neg, axis=0)\n        X_test_PCP = np.append(X_test_PCP_pos, X_test_PCP_neg, axis=0)\n        X_train_PCP, Y_train_PCP = shuffle(X_train_PCP, Y_train_PCP, random_state=42)\n        X_test_PCP, Y_test_PCP = shuffle(X_test_PCP, Y_test_PCP, random_state=42)\n\n        X_train_emb_neg = trainNeg_emb[train_index]\n        X_test_emb_neg = trainNeg_emb[test_index]\n        trainPos_emb = np.repeat(trainPos_emb, 10, axis=0)\n        X_train_emb_pos = trainPos_emb[:int(0.8 * len(trainPos_emb))]\n        X_test_emb_pos = trainPos_emb[int(0.8 * len(trainPos_emb)):]\n        Y_train_emb = np.append(np.ones(len(X_train_emb_pos)), np.zeros(len(X_train_emb_neg)), axis=0)\n        Y_test_emb = np.append(np.ones(len(X_test_emb_pos)), np.zeros(len(X_test_emb_neg)), axis=0)\n        X_train_emb = np.append(X_train_emb_pos, X_train_emb_neg, axis=0)\n        X_test_emb = np.append(X_test_emb_pos, X_test_emb_neg, axis=0)\n        X_train_emb, Y_train_emb = shuffle(X_train_emb, Y_train_emb, random_state=42)\n        X_test_emb, Y_test_emb = shuffle(X_test_emb, Y_test_emb, random_state=42)\n\n        X_train_DBPF_neg = trainNeg_DBPF[train_index]\n        X_test_DBPF_neg = trainNeg_DBPF[test_index]\n        trainPos_DBPF = np.repeat(trainPos_DBPF, 10, axis=0)\n        X_train_DBPF_pos = trainPos_DBPF[:int(0.8 * len(trainPos_DBPF))]\n        X_test_DBPF_pos = trainPos_DBPF[int(0.8 * len(trainPos_DBPF)):]\n        Y_train_DBPF = np.append(np.ones(len(X_train_DBPF_pos)), np.zeros(len(X_train_DBPF_neg)), axis=0)\n        Y_test_DBPF = np.append(np.ones(len(X_test_DBPF_pos)), np.zeros(len(X_test_DBPF_neg)), axis=0)\n        X_train_DBPF = np.append(X_train_DBPF_pos, X_train_DBPF_neg, axis=0)\n        X_test_DBPF = np.append(X_test_DBPF_pos, X_test_DBPF_neg, axis=0)\n        X_train_DBPF, Y_train_DBPF = shuffle(X_train_DBPF, Y_train_DBPF, random_state=42)\n        X_test_DBPF, Y_test_DBPF = shuffle(X_test_DBPF, Y_test_DBPF, random_state=42)\n\n        X_train_PSNP_neg = trainNeg_PSNP[train_index]\n        X_test_PSNP_neg = trainNeg_PSNP[test_index]\n        trainPos_PSNP = np.repeat(trainPos_PSNP, 10, axis=0)\n        X_train_PSNP_pos = trainPos_PSNP[:int(0.8 * len(trainPos_PSNP))]\n        X_test_PSNP_pos = trainPos_PSNP[int(0.8 * len(trainPos_PSNP)):]\n        Y_train_PSNP = np.append(np.ones(len(X_train_PSNP_pos)), np.zeros(len(X_train_PSNP_neg)), axis=0)\n        Y_test_PSNP = np.append(np.ones(len(X_test_PSNP_pos)), np.zeros(len(X_test_PSNP_neg)), axis=0)\n        X_train_PSNP = np.append(X_train_PSNP_pos, X_train_PSNP_neg, axis=0)\n        X_test_PSNP = np.append(X_test_PSNP_pos, X_test_PSNP_neg, axis=0)\n        X_train_PSNP, Y_train_PSNP = shuffle(X_train_PSNP, Y_train_PSNP, random_state=42)\n        X_test_PSNP, Y_test_PSNP = shuffle(X_test_PSNP, Y_test_PSNP, random_state=42)\n\n        X_train = np.append(X_train_PCP, X_train_PSNP, axis=1)\n        X_train = np.append(X_train, X_train_DBPF, axis=1)\n        X_train = np.append(X_train, X_train_emb, axis=1)\n\n        X_test = np.append(X_test_PCP, X_test_PSNP, axis=1)\n        X_test = np.append(X_test, X_test_DBPF, axis=1)\n        X_test = np.append(X_test, X_test_emb, axis=1)\n\n        x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n        x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n        y_train_tensor = torch.tensor(Y_train_PCP, dtype=torch.long)\n        y_test_tensor = torch.tensor(Y_test_PCP, dtype=torch.long)\n\n        train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n        test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n        print(\"The model is building the RNN_AE model now!\")\n        model = Model.RNN_AE(input_size=x_train_tensor.shape[1])\n        model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)\n\n        best_val_acc = 0.0\n        patience_counter = 0\n        for epoch in range(max_epochs):\n            model.train()\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in train_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                _, preds = torch.max(outputs, 1)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(train_loader.dataset)\n            epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n            model.eval()\n            val_loss = 0.0\n            val_corrects = 0\n            with torch.no_grad():\n                for inputs, labels in test_loader:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    val_loss += loss.item() * inputs.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    val_corrects += torch.sum(preds == labels.data)\n\n            val_loss /= len(test_loader.dataset)\n            val_acc = val_corrects.double() / len(test_loader.dataset)\n\n            print(\n                f\"Epoch {epoch + 1}/{max_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n            scheduler.step(val_loss)\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                patience_counter = 0\n                best_model_wts = copy.deepcopy(model.state_dict())\n            else:\n                patience_counter += 1\n\n            if patience_counter >= max_patience:\n                break\n\n        model.load_state_dict(best_model_wts)\n        model.eval()\n        test_corrects = 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                test_corrects += torch.sum(preds == labels.data)\n\n        test_acc = test_corrects.double() / len(test_loader.dataset)\n        print(f\"Test Accuracy: {test_acc:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}