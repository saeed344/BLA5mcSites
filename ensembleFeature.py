{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#import torch\nimport os\nimport math\nfrom collections import Counter\nfrom itertools import product\nimport collections\nfrom sklearn.utils import shuffle\nimport itertools\nfrom gensim.models import word2vec\n\n\n#NCPA\n#shape: batch_Size*input_length*4\ndef calculate(sequence):\n    X = []\n    dictNum = {'A' : 0, 'U' : 0, 'C' : 0, 'G' : 0, 'N': 0};\n    for i in range(len(sequence)):\n        if sequence[i] in dictNum.keys():\n            dictNum[sequence[i]] += 1;\n            X.append(dictNum[sequence[i]] / float(i + 1));\n    return np.array(X)\n\ndef NCPA(sequences):\n    chemical_property = {\n        'A': [1, 1, 1],\n        'U': [0, 1, 0],\n        'G': [1, 0, 0],\n        'C': [0, 0, 1]\n    }\n    ncp_feature = []\n    for seq in sequences:\n        ncp = []\n        for aaindex, aa in enumerate(seq):\n            ncp.append(chemical_property.get(aa, [0, 0, 0]))\n        ncpa=np.append(ncp,calculate(seq).reshape(-1,1),axis=1)\n        ncp_feature.append(ncpa)\n    return np.array(ncp_feature)\n\n#PSNP\ndef CalculateMatrix(data, order, k):\n    if k == 1:        \n        matrix = np.zeros((len(data[0]), 4))\n        for i in range(len(data[0])): \n            for j in range(len(data)):  \n                matrix[i][order[data[j][i:i+1]]] += 1     \n    return matrix\n\ndef PSNP(trainPos,trainNeg,testPos,testNeg,k=1):\n    #String\n    train_positive = []\n    for pos in trainPos:\n        train_positive.append(str(pos))\n    train_negative = []\n    for neg in trainNeg:\n        train_negative.append(str(neg))\n    train_p_num = len(train_positive)\n    train_n_num = len(train_negative)\n    \n    test_positive = []\n    for pos in testPos:\n        test_positive.append(str(pos))\n    test_negative = []\n    for neg in testNeg:\n        test_negative.append(str(neg))\n   \n    test_p_num = len(test_positive)\n    test_n_num = len(test_negative)\n    \n    test_lp = len(test_positive[0])\n    test_ln = len(test_negative[0])\n    \n    nucleotides = ['A', 'C', 'G', 'U'] \n    \n    if k == 1 :\n        nuc = [n1 for n1 in nucleotides]\n        order = {}\n        for i in range(len(nuc)):\n            order[nuc[i]] = i \n        matrix_po = CalculateMatrix(train_positive, order, 1) \n        matrix_ne = CalculateMatrix(train_negative, order, 1)\n\n        F1 = matrix_po/train_p_num \n        F2 = matrix_ne/train_n_num       \n        F = F1 - F2  \n\n        testPosCode = []\n        for sequence in test_positive:  \n            for j in range(len(sequence)):                \n                po_number = F[j][order[sequence[j:j+1]]]\n                testPosCode.append(po_number)  \n        testPosCode = np.array(testPosCode)\n        testPosCode = testPosCode.reshape((test_p_num,test_lp)) \n        \n        testNegCode = []    \n        for sequence in test_negative:    \n            for i in range(len(sequence)):\n                ne_number = F[i][order[sequence[i:i+1]]]\n                testNegCode.append(ne_number)  \n        testNegCode = np.array(testNegCode)\n        testNegCode = testNegCode.reshape((test_n_num,test_ln))  \n        \n        trainPosCode = []    \n        for sequence in train_positive:    \n            for i in range(len(sequence)):\n                po_number = F[i][order[sequence[i:i+1]]]\n                trainPosCode.append(po_number)  \n        trainPosCode = np.array(trainPosCode)\n        trainPosCode = trainPosCode.reshape((train_p_num,test_ln)) \n        \n        trainNegCode = []    \n        for sequence in train_negative:    \n            for i in range(len(sequence)):\n                ne_number = F[i][order[sequence[i:i+1]]]\n                trainNegCode.append(ne_number)  \n        trainNegCode = np.array(trainNegCode)\n        trainNegCode = trainNegCode.reshape((train_n_num,test_ln))\n        \n    return trainPosCode,trainNegCode,testPosCode,testNegCode\n\n#batch_Size*input_length*4\ndef Binary(sequences):\n    AA = 'ACGU'\n    binary_feature = []\n    for seq in sequences:\n        # seq=str(seq)[2:23]\n        binary = []\n        for aa in seq:\n            binary_one = []\n            for aa1 in AA:\n                tag = 1 if aa == aa1 else 0\n                binary_one.append(tag)\n            #binary.append(binary_one)\n            binary.append(binary_one)\n        binary_feature.append(binary)\n    return np.array(binary_feature)\n\n#word2vec\ndef emb_seqs(sequences, features=100, num = 4):\n    w2v_model = word2vec.Word2Vec.load(\"/kaggle/input/5mcsite-dnn/dna_w2v_100.pt\")\n    seqs_emb = []\n    for seq in sequences:\n        seq_emb = []\n        for i in range(len(seq) - num + 1):\n            try:\n                seq_emb.append(np.array(w2v_model.wv[seq[i:i+num]]))\n            except:\n                seq_emb.append(np.array(np.zeros([features])))\n        seqs_emb.append(seq_emb)\n    seqs_emb = np.array(seqs_emb).reshape(len(seqs_emb),-1,features)\n    return seqs_emb #\n\n#batch_size*(input_length-window+1)*4\ndef ENAC(sequences):\n    AA = 'ACGT'\n    enac_feature = []\n    window = 5\n    for seq in sequences:\n        #seq=str(seq)[2:23]\n        l = len(seq)\n        enac = []\n        for i in range(0, l):\n            if i < l and i + window <= l:\n                enac_one = []\n                count = Counter(seq[i:i + window])\n                for key in count:\n                    count[key] = count[key] / len(seq[i:i + window])\n                for aa in AA:\n                    enac_one.append(count[aa])\n                    #enac+=count[aa]\n                enac.append(enac_one)  #\n                #enac += enac_one  #\n        enac_feature.append(enac)\n    return np.array(enac_feature)\n\ndef PseDNC(sequences):\n    gene_type='DNA'\n    fill_NA='0'\n    propertyname=r\"/kaggle/input/5mcsite-dnn/physical_chemical_properties_DNA.txt\"\n    \n    phisical_chemical_proporties=pd.read_csv(propertyname,header=None,index_col=None)\n    \n    DNC_key=phisical_chemical_proporties.values[:,0]\n    if fill_NA==\"1\":\n        DNC_key[21]='NA'\n    \n    DNC_value=phisical_chemical_proporties.values[:,1:]\n    DNC_value=np.array(DNC_value).T  #\n    DNC_value_scale=[[]]*len(DNC_value)  \n    for i in range(len(DNC_value)):\n        average_=sum(DNC_value[i]*1.0/len(DNC_value[i]))  #\n        std_=np.std(DNC_value[i],ddof=1)  #\n        DNC_value_scale[i]=[round((e-average_)/std_,2) for e in DNC_value[i]]  #\n    DNC_value_scale=list(zip(*DNC_value_scale))  #\n\n    DNC_len=len(DNC_value_scale)  #\n    \n    w=0.9\n    Lamda=6  #\n    result_value=[]\n    m6a_len=len(sequences[0])  #\n    \n    m6a_num=len(sequences)  #\n    for m6a_line_index in range(m6a_num):  #\n        frequency=[0]*len(DNC_key)  #\n        #print len(frequency)\n        m6a_DNC_value=[[]]*(m6a_len-1)  #\n        #print m6a_DNC_value\n        for m6a_line_doublechar_index in range(m6a_len):\n            for DNC_index in range(len(DNC_key)):\n                if sequences[m6a_line_index][m6a_line_doublechar_index:m6a_line_doublechar_index+2]==DNC_key[DNC_index]:\n                    #print m6aseq[2][0:2]\n                    m6a_DNC_value[m6a_line_doublechar_index]=DNC_value_scale[DNC_index]  #\n                    frequency[DNC_index]+=1  #\n        #print m6a_DNC_value\n\n        frequency=[e/float(sum(frequency)) for e in frequency]  #\n        p=sum((frequency))  #\n        \n        one_line_value_with = 0.0\n        sita = [0] * Lamda  #\n        for lambda_index in range(1, Lamda + 1):\n            one_line_value_without_ = 0.0\n            for m6a_sequence_value_index in range(1, m6a_len - lambda_index):\n                temp = list(map(lambda x,y : round((x - y) ** 2,8), list(np.array(m6a_DNC_value[m6a_sequence_value_index - 1])),list(np.array(m6a_DNC_value[m6a_sequence_value_index - 1 + lambda_index]))))\n\n                temp_value = round(sum(temp) * 1.0 / DNC_len,8)\n                one_line_value_without_ += temp_value\n            one_line_value_without_ = round(one_line_value_without_ / (m6a_len - lambda_index-1),8)\n            sita[lambda_index - 1] = one_line_value_without_\n            one_line_value_with += one_line_value_without_\n        dim = [0] * (len(DNC_key) + Lamda)\n        for index in range(1, len(DNC_key) + Lamda+1):\n            if index <= len(DNC_key):\n                dim[index - 1] = frequency[index - 1] / (1.0 + w * one_line_value_with)\n            else:\n                dim[index - 1] = w * sita[index - len(DNC_key)-1] / (1.0 + w * one_line_value_with)\n            dim[index-1]=round(dim[index-1],8)\n        result_value.append(dim)\n    return np.array(result_value)\n\ndef query(short,sequence):\n    count = 0\n    for i in range(len(sequence)-len(short)+1):\n        if sequence[i:i+len(short)]==short:\n            count+=1\n    return count\ndef DNC(sequences):\n    final = []\n    for seq in sequences:\n        seq_length = len(seq)-1\n        RNA = ['A', 'U', 'C', 'G']\n        di_nucleotide_values = []\n        di_nucleotide_dict = {\"\".join(i): 0 for i in product(RNA, repeat=2)}\n        for di_nucleotide in di_nucleotide_dict.keys():\n            di_nucleotide_dict[di_nucleotide] = round(query(di_nucleotide,seq) / seq_length, 3)\n        for dict_value in di_nucleotide_dict.values():\n            di_nucleotide_values.append(dict_value)\n        final.append(di_nucleotide_values)\n    return np.array(final)\n\ndef EIIP(sequences):\n    dic = {\"A\": 0.1260,\"C\": 0.1340,\"G\": 0.0806,\"T\": 0.1335}\n    result = []\n    for seq in sequences:\n        result_one = []\n        for k in seq:\n            result_one.append(dic[k])\n        result.append(result_one)\n    return np.array(result)\n\n# def PCP(sequences):\n#     path = \"\"\n#     gene_type = \"DNA\"\n#     fill_NA = '0'\n#     propertyname = \"/kaggle/input/5mcsite-dnn/physical_chemical_properties_DNA.txt\"  # Adjust this path as needed\n#     physical_chemical_properties_path=propertyname\n\n#     data=pd.read_csv(physical_chemical_properties_path,header=None,index_col=None)#read the phisical chemichy proporties\n#     prop_key=data.values[:,0]\n\n#     if fill_NA==\"1\":\n#         prop_key[21]='NA'\n#     prop_data=data.values[:,1:]\n#     prop_data=np.matrix(prop_data)\n#     DNC_value=np.array(prop_data).T\n#     DNC_value_scale=[[]]*len(DNC_value)\n#     for i in list(range(len(DNC_value))):\n#         average_=sum(DNC_value[i]*1.0/len(DNC_value[i]))\n#         std_=np.std(DNC_value[i],ddof=1)\n#         DNC_value_scale[i]=[round((e-average_)/std_,2) for e in DNC_value[i]]\n#     prop_data_transformed=list(zip(*DNC_value_scale))\n#     prop_len=len(prop_data_transformed[0])\n\n#     whole_m6a_seq=sequences\n#     i=0\n#     phisical_chemichy_len=len(prop_data_transformed)#the length of properties\n#     sequence_line_len=len(sequences[0])#the length of one sequence\n#     LAMDA=4\n#     finally_result=[]#used to save the fanal result\n#     for one_m6a_sequence_line in whole_m6a_seq:\n# #         one_sequence_value=[[]]*(sequence_line_len-1)\n# #         PC_m=[0.0]*prop_len\n# #         PC_m=np.array(PC_m)\n# #         for one_sequence_index in range(sequence_line_len-1):\n# #             for prop_index in list(range(len(prop_key))):\n# #                 if one_m6a_sequence_line[one_sequence_index:one_sequence_index+2]==prop_key[prop_index]:\n# #                     one_sequence_value[one_sequence_index]=prop_data_transformed[prop_index]\n# #             PC_m+=np.array(one_sequence_value[one_sequence_index])\n# #         PC_m=PC_m/(sequence_line_len-1)\n#         sequence_line_len = len(one_m6a_sequence_line)\n#         PC_m = np.zeros(prop_len)\n#         one_sequence_value = [0] * (sequence_line_len - 1)  # Initialize as list of zeros\n        \n#         for one_sequence_index in range(sequence_line_len - 1):\n#             for prop_index in range(len(prop_key)):\n#                 if one_m6a_sequence_line[one_sequence_index:one_sequence_index + 2] == prop_key[prop_index]:\n#                     one_sequence_value[one_sequence_index] = prop_data_transformed[prop_index]\n\n#             PC_m += np.array(one_sequence_value[one_sequence_index])\n\n#         PC_m = PC_m / (sequence_line_len - 1)\n#         auto_value=[]\n#         for LAMDA_index in list(range(1,LAMDA+1)):\n#             temp = [0.0] * prop_len\n#             temp=np.array(temp)\n#             for auto_index in list(range(1,sequence_line_len-LAMDA_index)):\n#                 temp=temp+(np.array(one_sequence_value[auto_index-1])-PC_m)*(np.array(one_sequence_value[auto_index+LAMDA_index-1])-PC_m)\n#                 temp=[round(e,8) for e in temp.astype(float)]\n#             x=[round(e/(sequence_line_len-LAMDA_index-1),8) for e in temp]\n#             auto_value.extend([round(e,8) for e in x])\n#         for LAMDA_index in list(range(1, LAMDA + 1)):\n#             for i in list(range(1,prop_len+1)):\n#                 for j in list(range(1,prop_len+1)):\n#                     temp2=0.0\n#                     if i != j:\n#                         for auto_index in list(range(1, sequence_line_len - LAMDA_index)):\n#                                 temp2+=(one_sequence_value[auto_index-1][i-1]-PC_m[i-1])*(one_sequence_value[auto_index+LAMDA_index-1][j-1]-PC_m[j-1])\n#                         auto_value.append(round(temp2/((sequence_line_len-1)-LAMDA_index),8))\n\n#         finally_result.append(auto_value)\n#     return np.array(finally_result)#.reshape(len(finally_result),-1,1)\n\n\ndef DBPF(sequences):\n    DB = []\n    for sequence in sequences:\n        alphabet=\"ACGU\"\n        k_num=2\n        two_sequence=[]\n        for index,data in enumerate(sequence):\n            if index <(len(sequence)-k_num+1):\n                two_sequence.append(\"\".join(sequence[index:(index+k_num)]))\n        parameter=[e for e in itertools.product([0,1],repeat=4)]\n        record=[0 for x in range(int(pow(4,k_num)))]\n        matrix=[\"\".join(e) for e in itertools.product(alphabet, repeat=k_num)] # AA AU AC AG UU UC ...\n        final=[]\n        for index,data in enumerate(two_sequence):\n            final_one = []\n            if data in matrix:\n                final_one.extend(parameter[matrix.index(data)])\n                record[matrix.index(data)]+=1\n                final_one.append(record[matrix.index(data)]*1.0/(index+1))\n            final.append(final_one)\n        DB.append(final)\n    return np.array(DB)","metadata":{"_uuid":"b112fb11-1836-483f-bf99-3e01fc50e75d","_cell_guid":"f6b4253f-a490-4888-8c5b-10c46b593a8c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-01T10:23:04.604142Z","iopub.execute_input":"2024-07-01T10:23:04.604479Z","iopub.status.idle":"2024-07-01T10:23:15.825207Z","shell.execute_reply.started":"2024-07-01T10:23:04.604454Z","shell.execute_reply":"2024-07-01T10:23:15.824418Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def PCP(sequences):\n    path = \"\"\n    gene_type = \"DNA\"\n    fill_NA = '0'\n    propertyname = \"/kaggle/input/5mcsite-dnn/physical_chemical_properties_DNA.txt\"  # Adjust this path as needed\n    physical_chemical_properties_path=propertyname\n\n    data=pd.read_csv(physical_chemical_properties_path,header=None,index_col=None)#read the phisical chemichy proporties\n    prop_key=data.values[:,0]\n\n    if fill_NA==\"1\":\n        prop_key[21]='NA'\n    prop_data=data.values[:,1:]\n    prop_data=np.matrix(prop_data)\n    DNC_value=np.array(prop_data).T\n    DNC_value_scale=[[]]*len(DNC_value)\n    for i in list(range(len(DNC_value))):\n        average_=sum(DNC_value[i]*1.0/len(DNC_value[i]))\n        std_=np.std(DNC_value[i],ddof=1)\n        DNC_value_scale[i]=[round((e-average_)/std_,2) for e in DNC_value[i]]\n    prop_data_transformed=list(zip(*DNC_value_scale))\n    prop_len=len(prop_data_transformed[0])\n\n    whole_m6a_seq=sequences\n    i=0\n    phisical_chemichy_len=len(prop_data_transformed)#the length of properties\n    sequence_line_len=len(sequences[0])#the length of one sequence\n    LAMDA=4\n    finally_result=[]#used to save the fanal result\n    for one_m6a_sequence_line in whole_m6a_seq:\n        one_sequence_value=[[]]*(sequence_line_len-1)\n        PC_m=[0.0]*prop_len\n        PC_m=np.array(PC_m)\n        for one_sequence_index in range(sequence_line_len-1):\n            for prop_index in list(range(len(prop_key))):\n                if one_m6a_sequence_line[one_sequence_index:one_sequence_index+2]==prop_key[prop_index]:\n                    one_sequence_value[one_sequence_index]=prop_data_transformed[prop_index]\n            PC_m+=np.array(one_sequence_value[one_sequence_index])\n        PC_m=PC_m/(sequence_line_len-1)\n#         sequence_line_len = len(one_m6a_sequence_line)\n#         PC_m = np.zeros(prop_len)\n#         one_sequence_value = [0] * (sequence_line_len - 1)  # Initialize as list of zeros\n        \n#         for one_sequence_index in range(sequence_line_len - 1):\n#             for prop_index in range(len(prop_key)):\n#                 if one_m6a_sequence_line[one_sequence_index:one_sequence_index + 2] == prop_key[prop_index]:\n#                     one_sequence_value[one_sequence_index] = prop_data_transformed[prop_index]\n\n#             PC_m += np.array(one_sequence_value[one_sequence_index])\n\n#         PC_m = PC_m / (sequence_line_len - 1)\n        auto_value=[]\n        for LAMDA_index in list(range(1,LAMDA+1)):\n            temp = [0.0] * prop_len\n            temp=np.array(temp)\n            for auto_index in list(range(1,sequence_line_len-LAMDA_index)):\n                temp=temp+(np.array(one_sequence_value[auto_index-1])-PC_m)*(np.array(one_sequence_value[auto_index+LAMDA_index-1])-PC_m)\n                temp=[round(e,8) for e in temp.astype(float)]\n            x=[round(e/(sequence_line_len-LAMDA_index-1),8) for e in temp]\n            auto_value.extend([round(e,8) for e in x])\n        for LAMDA_index in list(range(1, LAMDA + 1)):\n            for i in list(range(1,prop_len+1)):\n                for j in list(range(1,prop_len+1)):\n                    temp2=0.0\n                    if i != j:\n                        for auto_index in list(range(1, sequence_line_len - LAMDA_index)):\n                                temp2+=(one_sequence_value[auto_index-1][i-1]-PC_m[i-1])*(one_sequence_value[auto_index+LAMDA_index-1][j-1]-PC_m[j-1])\n                        auto_value.append(round(temp2/((sequence_line_len-1)-LAMDA_index),8))\n\n        finally_result.append(auto_value)\n    return np.array(finally_result)#.reshape(len(finally_result),-1,1)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_fasta(fasta_file_path):\n    sequences = []\n    names = []\n    with open(fasta_file_path, 'r') as fasta_file:\n        for line in fasta_file:\n            if line.startswith('>'):\n                name = line.strip()[1:]\n                names.append(name)\n                sequence = ''\n            else:\n                sequence += line.strip()\n                sequences.append(sequence)\n    return sequences, names\ndef PCP(sequences):\n    propertyname = \"/kaggle/input/5mcsite-dnn/physical_chemical_properties_DNA.txt\"  # Adjust this path as needed\n\n    data = pd.read_csv(propertyname, header=None, index_col=None)\n    prop_key = data.values[:, 0]\n\n    prop_data = data.values[:, 1:]\n    prop_data = np.matrix(prop_data)\n    DNC_value = np.array(prop_data).T\n\n    DNC_value_scale = [[]] * len(DNC_value)\n    for i in range(len(DNC_value)):\n        average_ = sum(DNC_value[i] * 1.0 / len(DNC_value[i]))\n        std_ = np.std(DNC_value[i], ddof=1)\n        DNC_value_scale[i] = [round((e - average_) / std_, 2) for e in DNC_value[i]]\n\n    prop_data_transformed = list(zip(*DNC_value_scale))\n    prop_len = len(prop_data_transformed[0])\n\n    LAMDA = 4\n    finally_result = []\n\n    for one_m6a_sequence_line in sequences:\n        sequence_line_len = len(one_m6a_sequence_line)\n        PC_m = np.zeros(prop_len)\n        one_sequence_value = np.zeros((sequence_line_len - 1, prop_len))  # Initialize as zeros\n\n        for one_sequence_index in range(sequence_line_len - 1):\n            for prop_index in range(len(prop_key)):\n                if one_m6a_sequence_line[one_sequence_index:one_sequence_index + 2] == prop_key[prop_index]:\n                    one_sequence_value[one_sequence_index] = prop_data_transformed[prop_index]\n\n            PC_m += one_sequence_value[one_sequence_index]\n\n        PC_m = PC_m / (sequence_line_len - 1)\n        auto_value = []\n\n        for LAMDA_index in range(1, LAMDA + 1):\n            temp = np.zeros(prop_len)\n\n            for auto_index in range(1, sequence_line_len - LAMDA_index):\n                temp += (one_sequence_value[auto_index - 1] - PC_m) * (\n                            one_sequence_value[auto_index + LAMDA_index - 1] - PC_m)\n\n            if (sequence_line_len - LAMDA_index - 1) != 0:\n                x = temp / (sequence_line_len - LAMDA_index - 1)\n                auto_value.extend(x.tolist())\n\n        for LAMDA_index in range(1, LAMDA + 1):\n            for i in range(1, prop_len + 1):\n                for j in range(1, prop_len + 1):\n                    temp2 = 0.0\n                    if i != j:\n                        for auto_index in range(1, sequence_line_len - LAMDA_index):\n                            temp2 += (one_sequence_value[auto_index - 1][i - 1] - PC_m[i - 1]) * (\n                                        one_sequence_value[auto_index + LAMDA_index - 1][j - 1] - PC_m[j - 1])\n\n                        if ((sequence_line_len - 1) - LAMDA_index) != 0:\n                            auto_value.append(round(temp2 / ((sequence_line_len - 1) - LAMDA_index), 8))\n\n        finally_result.append(auto_value)\n\n    return np.array(finally_result)\n# Assuming these lines are correctly aligned with the function definition\ntrain_seq, name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_train.fasta\")\n\ntrainPos_seq = []\ntrainNeg_seq = []\nfor i in range(len(name)):\n    if name[i][0] == 'P':\n        trainPos_seq.append(train_seq[i])\n    else:\n        trainNeg_seq.append(train_seq[i])\n\ntest_seq, name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_ind.fasta\")\n\ntestPos_seq = []\ntestNeg_seq = []\nfor i in range(len(name)):\n    if name[i][0] == 'P':\n        testPos_seq.append(test_seq[i])\n    else:\n        testNeg_seq.append(test_seq[i])\n\ntrainPos_PCP,_ = PCP(trainPos_seq)\ntrainNeg_PCP,_ = PCP(trainNeg_seq)\ntestPos_PCP ,_= PCP(testPos_seq)\ntestNeg_PCP,_ = PCP(testNeg_seq)\n\nprint(f\"PCP features shape: {trainPos_PCP.shape}\")\nprint(f\"PCP features shape: {trainNeg_PCP.shape}\")\nprint(f\"PCP features shape: {testPos_PCP.shape}\")\nprint(f\"PCP features shape: {testNeg_PCP.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T11:10:03.699418Z","iopub.execute_input":"2024-07-01T11:10:03.700266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve,auc,roc_auc_score\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n# import sys\n# sys.path.append(\"../\")\n# from Resnet import Res_Net\n#import tqdm\n\n#calculate the auc score\ndef auc_cal(probs, targets):\n    if isinstance(probs, torch.Tensor) and isinstance(targets, torch.Tensor):\n        fpr, tpr, thresholds = roc_curve(y_true=targets.detach().cpu().numpy(),\n                                         y_score=probs.detach().cpu().numpy()) \n    elif isinstance(probs, np.ndarray) and isinstance(targets, np.ndarray):\n         fpr, tpr, thresholds = roc_curve(y_true=targets,y_score=probs)\n    else:\n        print('ERROR: probs or targets type is error.')\n        raise TypeError\n    auc_ = auc(x=fpr, y=tpr)\n    return auc_\n\n#draw the auroc curve\ndef auc_curve(prob,y):\n    fpr, tpr, threshold = roc_curve(y, prob)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()\n    lw = 2\n    plt.figure(figsize=(10, 10))\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    #plt.savefig(\"BERT.png\")\n    #plt.show()\n\n#test data and return metrics of results \ndef evaluate(model, dataloader, device, is_train=True, threshold=0.5):\n    model.eval()\n    y_true = torch.tensor([],dtype=torch.int)\n    y_score = torch.tensor([])\n    #for data in tqdm(dataloader):\n    model = model.to(device)\n    for data in dataloader:\n        #if not isinstance(model, Res_Net):\n        if 1==1:\n            inputs,y = data\n            inputs = inputs.to(device)\n            out = model(inputs)\n        out = out.squeeze(dim=-1)\n        #out = torch.sigmoid(out)\n        y_true = torch.cat((y_true, y.int().detach().cpu()))\n        y_score = torch.cat((y_score, out.detach().cpu()))\n    y_true = y_true.numpy()\n    y_score = y_score.numpy()\n    if is_train:\n        return get_train_metrics(y_score, y_true)\n    else:\n        return get_test_metrics(y_score, y_true, threshold)\n\n#evaluate model's results\ndef evaluate_result(model, dataloader, device, is_train=True, threshold=0.5):\n    model.eval()\n    y_true = torch.tensor([],dtype=torch.int)\n    y_score = torch.tensor([])\n    #for data in tqdm(dataloader):\n    model = model.to(device)\n    for data in dataloader:\n        #if not isinstance(model, Res_Net):\n        if 1==1:\n            inputs,y = data\n            inputs = inputs.to(device)\n            out = model(inputs)\n        out = out.squeeze(dim=-1)\n        #out = torch.sigmoid(out)\n        y_true = torch.cat((y_true, y.int().detach().cpu()))\n        y_score = torch.cat((y_score, out.detach().cpu()))\n    y_true = y_true.numpy()\n    y_score = y_score.numpy()\n    return y_score, y_true\n\n#return the attention output and model's results\ndef evaluate_attn(model, dataloader, device, is_train=True, threshold=0.5):\n    model.eval()\n    y_true = torch.tensor([],dtype=torch.int)\n    y_score = torch.tensor([])\n    attn_all = torch.tensor(np.empty (shape= [0,41,41]))\n    #for data in tqdm(dataloader):\n    model = model.to(device)\n    for data in dataloader:\n        #if not isinstance(model, Res_Net):\n        if 1==1:\n            inputs,y = data\n            inputs = inputs.to(device)\n            out,attn = model(inputs)\n        out = out.squeeze(dim=-1)\n        #out = torch.sigmoid(out)\n        attn_all = torch.cat((attn_all, attn.detach().cpu()))\n        y_true = torch.cat((y_true, y.int().detach().cpu()))\n        y_score = torch.cat((y_score, out.detach().cpu()))\n    y_true = y_true.numpy()\n    y_score = y_score.numpy()\n    if is_train:\n        return attn_all,get_train_metrics(y_score, y_true)\n    else:\n        return attn_all,get_test_metrics(y_score, y_true, threshold)\n\n#calculate the metrics of the training dataset\ndef get_train_metrics(y_pred, y_true):\n    y_pred = np.array(y_pred)\n    y_true = np.array(y_true)\n\n    desc_score_indices = np.argsort(y_pred, kind=\"mergesort\")[::-1]\n    y_pred = y_pred[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n\n    TP = FP = 0\n    TN = np.sum(y_true == 0) \n    FN = np.sum(y_true == 1)  \n    mcc = 0\n    mcc_threshold = y_pred[0] + 1 \n    confuse_matrix = (TP, FP, TN, FN)  \n    max_mcc = -1 \n    \n    for index, score in enumerate(y_pred):  \n        if y_true[index] == 1:\n            TP += 1\n            FN -= 1\n        else:\n            FP += 1\n            TN -= 1\n        numerator = (TP * TN - FP * FN)\n        denominator = (math.sqrt(TP + FP) * math.sqrt(TN + FN) * math.sqrt(TP + FN) * math.sqrt(TN + FP))\n        if denominator == 0:\n            mcc = 0\n        else:\n            mcc = numerator / denominator\n\n        if mcc > max_mcc:\n            max_mcc = mcc\n            confuse_matrix = (TP, FP, TN, FN)\n            mcc_threshold = score \n    TP, FP, TN, FN = confuse_matrix \n    Sen = 0 if (TP + FN) == 0 else (TP / (TP + FN))  \n    Spe = 0 if (TN + FP) == 0 else (TN / (TN + FP))\n    Acc = 0 if (TP + FP + TN + FN) == 0 else ((TP + TN) / (TP + FP + TN + FN))\n    AUC = roc_auc_score(y_true, y_pred)\n    return mcc_threshold, TN, FN, FP, TP, Sen, Spe, Acc, max_mcc, AUC \n\n#calculate the metrics and suitable threshold of test results\ndef get_test_metrics(y_pred, y_true, threshold):\n    # print(threshold)\n    # print(y_pred)\n    y_pred = np.array(y_pred)\n    y_true = np.array(y_true)\n    TP = TN = FP = FN = 0\n    for i in range(len(y_true)):\n        if y_true[i] == 1 and y_pred[i] >= threshold:\n            TP += 1\n        elif y_true[i] == 1 and y_pred[i] < threshold:\n            FN += 1\n        elif y_true[i] == 0 and y_pred[i] >= threshold:\n            FP += 1\n        elif y_true[i] == 0 and y_pred[i] < threshold:\n            TN += 1\n    Sen = 0 if (TP + FN) == 0 else (TP / (TP + FN))\n    Spe = 0 if (TN + FP) == 0 else (TN / (TN + FP))\n    Acc = 0 if (TP + FP + TN + FN) == 0 else ((TP + TN) / (TP + FP + TN + FN))\n    AUC = roc_auc_score(y_true, y_pred)\n    #AUC = auc_cal(y_pred,y_true)\n    #auc_curve(y_pred,y_true)\n    numerator = (TP * TN - FP * FN)\n    denominator = (math.sqrt(TP + FP) * math.sqrt(TN + FN) * math.sqrt(TP + FN) * math.sqrt(TN + FP))\n    if denominator == 0:\n        mcc = 0\n    else:\n        mcc = numerator / denominator\n    return TN, FN, FP, TP, Sen, Spe, Acc, mcc, AUC","metadata":{"execution":{"iopub.status.busy":"2024-07-01T11:08:18.009241Z","iopub.execute_input":"2024-07-01T11:08:18.009545Z","iopub.status.idle":"2024-07-01T11:08:19.755746Z","shell.execute_reply.started":"2024-07-01T11:08:18.009520Z","shell.execute_reply":"2024-07-01T11:08:19.754807Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport re\n#from sklearn.utils import shuffle\n# from utils import metricsCal\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom sklearn.svm import SVC\nimport math\nimport sys\nimport copy\nimport pickle\n#from torch.autograd import Variable\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport os\n\n#this is attention module.\ndef attention(query, key, value, mask=None, dropout=None):  # q,k,v: [batch, h, seq_len, d_k]\n    d_k = query.size(-1)  # dim of query\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  #[batch, h, seq_len, seq_len]\n    p_atten = F.softmax(scores, dim=-1)  #[batch, h, seq_len, seq_len]\n    if dropout is not None:\n        p_atten = dropout(p_atten)\n    return torch.matmul(p_atten, value), p_atten\n\n#this is position encoding scheme.\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, dim1, dim2, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n\n        #if dim % 2 != 0:\n        #    raise ValueError(\"Cannot use sin/cos positional encoding with \"\n        #                     \"odd dim (got dim={:d})\".format(dim))\n\n        \"\"\"\n        PE(pos,2i/2i+1) = sin/cos(pos/10000^{2i/d_{model}})\n        \"\"\"\n        pe = torch.zeros(max_len, dim2)  #\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term0 = torch.exp((torch.arange(0, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n        div_term1 = torch.exp((torch.arange(1, dim2, 2, dtype=torch.float) * -(math.log(10000.0) / dim2)))\n        \n        pe[:, 0::2] = torch.sin(position.float() * div_term0)\n        pe[:, 1::2] = torch.cos(position.float() * div_term1)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        #self.drop_out = nn.Dropout(p=dropout)\n        self.dim2 = dim2\n        self.bm1 = nn.BatchNorm1d(dim1,eps=1e-05)\n\n    def forward(self, emb, step=None):\n        emb = emb * math.sqrt(self.dim2)\n        if step is None:\n            #emb = torch.tensor(emb) + self.pe[:,:emb.shape[1]]\n            emb = emb.clone().detach().requires_grad_(True) + self.pe[:,:emb.shape[1]]\n        else:\n            emb = emb + self.pe[step]\n        #emb = self.drop_out(emb)\n        emb = self.bm1(emb.to(torch.float32))\n        return emb\n\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module)\n                          for _ in range(N)])\n\n#this is self-attention module.\nclass SelfAttention(nn.Module):\n\n    def __init__(self,embedding_dim, dropout=0.1):\n        super(SelfAttention, self).__init__()\n        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n        nbatches = query.shape[0]\n        query, key, value = [\n            l(x) for l, x in zip(self.linears,\n                            (query.to(torch.float32),\n                             key.to(torch.float32),\n                             value.to(torch.float32) ) )\n        ]\n        attn, p_atten = attention(query,key,value,mask=mask,dropout=self.dropout)\n        out = self.linears[-1](attn)\n        return out,p_atten\n# This is MultiSelf-Attention Module.\nclass MultiSelfAttention(nn.Module):\n\n    def __init__(self, h,embedding_dim ,dropout=0.1):\n        super(MultiSelfAttention, self).__init__()\n        self.h = h\n        self.attn_modules = clones(SelfAttention(embedding_dim), h)\n\n    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n        for i in range(self.h):\n            \n            if i != 0:\n                out_one,attn_one= self.attn_modules[i](query,key,value)\n                # out = torch.add(out,torch.tensor(out_one))\n                # attn = torch.add(attn,torch.tensor(attn_one))\n                out = torch.add(out,out_one.clone().detach().requires_grad_(True))\n                attn = torch.add(attn,attn_one.clone().detach().requires_grad_(True))\n            else:\n                out,attn = self.attn_modules[i](query,key,value)\n\n        return out,attn\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, h, embedding_dim, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n        #assert embedding_dim % h == 0 \n        self.d_k = embedding_dim // h  #\n        self.h = h  \n        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)  #    \n        self.dropout = nn.Dropout(p=dropout)  #\n\n    def forward(self,query,key,value,mask=None):  # q,k,v: [batch, seq_len, embedding_dim]\n        #if mask is not None:\n        #    mask = mask.unsqueeze(1)  # [batch, seq_len, 1]\n        nbatches = query.shape[0]  #\n\n        query, key, value = [\n            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)  #\n            for l, x in zip(self.linears,\n                            (query.to(torch.float32),\n                             key.to(torch.float32), \n                             value.to(torch.float32) ) )\n        ]  #\n        attn, p_atten = attention(query,key,value,mask=mask,dropout=self.dropout)\n        # 3. \"Concat\" using a view and apply a final linear.\n        # [batch, h, seq_len, d_k]->[batch, seq_len, embedding_dim]\n        attn = attn.transpose(1,2).contiguous().view(nbatches, -1,self.h * self.d_k)\n        out = self.linears[-1](attn)\n        return out,attn\n    \nclass BahdanauAttention(nn.Module):\n    \"\"\"\n    input: from RNN module h_1, ... , h_n (batch_size, seq_len, units*num_directions),\n                                    h_n: (num_directions, batch_size, units)\n    return: (batch_size, num_task, units)\n    \"\"\"\n    def __init__(self,in_features, hidden_units,num_task):\n        super(BahdanauAttention,self).__init__()\n        self.W1 = nn.Linear(in_features=in_features,out_features=hidden_units)\n        self.W2 = nn.Linear(in_features=in_features,out_features=hidden_units)\n        self.V = nn.Linear(in_features=hidden_units, out_features=num_task)\n\n    def forward(self, hidden_states, values):\n        #hidden_with_time_axis = torch.unsqueeze(hidden_states,dim=1)\n        score  = self.V(nn.Tanh()(self.W1(values)+self.W2(hidden_states)))\n        attention_weights = nn.Softmax(dim=1)(score)\n        #print(attention_weights.shape)\n        values = torch.transpose(values,1,2)   # transpose to make it suitable for matrix multiplication\n        #print(attention_weights.shape,values.shape)\n        context_vector = torch.matmul(values,attention_weights)\n        context_vector = torch.transpose(context_vector,1,2)\n        return context_vector, attention_weights\n    \n#BiLSTM+Self-Attention\nclass ModelBS(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelBS, self).__init__()\n        self.posi = PositionalEncoding(dim1,dim2,dropout)\n        self.self_A = SelfAttention(dim2)\n        self.self_B = SelfAttention(dim1)\n        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.conv2 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        \n        x1 = self.conv1(x)\n        x1 = self.conv2(x1)  \n        x2 = self.posi(x)\n        \n        x2,(h_n,c_n) = self.lstm(x2)\n        if x2.shape[1] == 1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n            x2,attn = self.self_B(x2,x2,x2)\n        else:\n            x2,attn = self.self_A(x2,x2,x2)\n            \n        if x2.shape[2]==1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n        out = x1+x2\n        out = self.bm1(out)\n        #out = out.view(out.shape[0],-1)\n        return out,x2\n\nclass ModelBS_Pro(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelBS_Pro, self).__init__()\n        self.BS1 = ModelBS(dim1,dim2) \n        self.BS2 = ModelBS(dim1,dim2)\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.fn1 = nn.Linear(dim1*dim2,128)\n        self.fn2 = nn.Linear(128,1)\n        self.fn3 = nn.Linear(dim1*dim2,128)\n        self.fn4 = nn.Linear(128,1)\n        self.ac = nn.Sigmoid()\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        x1,x2 = self.BS1(x)\n        x1,x2 = self.BS2(x1)\n        x1 = self.bm1(x1)\n        x1 = self.conv1(x1)\n        x1 = x1.contiguous().view(x1.shape[0],-1)\n        x1 = self.fn1(x1)\n        x1 = self.fn2(x1)\n        \n        x2 = x2.contiguous().view(x2.shape[0],-1)\n        x2 = self.fn3(x2)\n        x2 = self.fn4(x2)\n        out = x1+x2      \n        out = self.ac(out)\n        return out\n\n#BiLSTM+MultiHead-Attention\nclass ModelB_Multi(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_Multi, self).__init__()\n        self.posi = PositionalEncoding(dim1,dim2,dropout)\n        self.self_A = MultiHeadAttention(10,dim2)\n        self.self_B = MultiHeadAttention(10,dim1)\n        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.conv2 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        \n        x1 = self.conv1(x)\n        x1 = self.conv2(x1)  \n        x2 = self.posi(x)\n        x2,(h_n,c_n) = self.lstm(x2)\n        if x2.shape[1] == 1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n            x2,attn = self.self_B(x2,x2,x2)\n        else:\n            x2,attn = self.self_A(x2,x2,x2)\n        if x2.shape[2]==1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n        out = x1+x2\n        out = self.bm1(out)\n        #out = out.view(out.shape[0],-1)\n        return out,x2\n    \nclass ModelB_Multi_Pro(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_Multi_Pro, self).__init__()\n        self.BS1 = ModelB_Multi(dim1,dim2)\n        self.BS2 = ModelB_Multi(dim1,dim2)\n\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.fn1 = nn.Linear(dim1*dim2,128)\n        self.fn2 = nn.Linear(128,1)\n        self.fn3 = nn.Linear(dim1*dim2,128)\n        self.fn4 = nn.Linear(128,1)\n        self.ac = nn.Sigmoid()\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        x1,x2 = self.BS1(x)\n        x1,x2 = self.BS2(x1)\n\n        x1 = self.bm1(x1)\n        x1 = self.conv1(x1)\n        x1 = x1.contiguous().view(x1.shape[0],-1)\n        x1 = self.fn1(x1)\n        x1 = self.fn2(x1)\n        \n        x2 = x2.contiguous().view(x2.shape[0],-1)\n        x2 = self.fn3(x2)\n        x2 = self.fn4(x2)\n        \n        out = x1+x2\n        \n        out = self.ac(out)\n        return out\n    \n#BiLSTM+Bah-Attention\nclass ModelB_Bah(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_Bah, self).__init__()\n        self.posi = PositionalEncoding(dim1,dim2,dropout)\n        self.self_A = BahdanauAttention(dim2,dim2,dim1)\n        self.self_B = BahdanauAttention(dim1,dim1,dim2)\n        self.lstm_A = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n        self.lstm_B = nn.LSTM(input_size=dim1,hidden_size=dim1,batch_first=True,bidirectional=False)\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.conv2 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        \n        x1 = self.conv1(x)\n        x1 = self.conv2(x1)  \n        x2 = self.posi(x)\n        \n        if x2.shape[1] == 1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n            x2,(h_n,c_n) = self.lstm_B(x2)\n            h_n = h_n.view(h_n.shape[1],h_n.shape[0],h_n.shape[2])\n            x2,attn = self.self_B(h_n,x2)\n        else:\n            x2,(h_n,c_n) = self.lstm_A(x2)\n            h_n = h_n.view(h_n.shape[1],h_n.shape[0],h_n.shape[2])\n            x2,attn = self.self_A(h_n,x2)\n        if x2.shape[2]==1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n        out = x1+x2\n        out = self.bm1(out)\n        #out = out.view(out.shape[0],-1)\n        return out,x2\n    \nclass ModelB_Bah_Pro(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_Bah_Pro, self).__init__()\n        self.BS1 = ModelB_Bah(dim1,dim2) \n        self.BS2 = ModelB_Bah(dim1,dim2)\n\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.fn1 = nn.Linear(dim1*dim2,128)\n        self.fn2 = nn.Linear(128,1)\n        self.fn3 = nn.Linear(dim1*dim2,128)\n        self.fn4 = nn.Linear(128,1)\n        self.ac = nn.Sigmoid()\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        x1,x2 = self.BS1(x)\n        x1,x2 = self.BS2(x1)\n\n        x1 = self.bm1(x1)\n        x1 = self.conv1(x1)\n        x1 = x1.contiguous().view(x1.shape[0],-1)\n        x1 = self.fn1(x1)\n        x1 = self.fn2(x1)\n        \n        x2 = x2.contiguous().view(x2.shape[0],-1)\n        x2 = self.fn3(x2)\n        x2 = self.fn4(x2)\n        \n        out = x1+x2\n        \n        out = self.ac(out)\n        return out\n\n#BiLSTM+MultiSelf-Attention\nclass ModelB_MultiSelf(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_MultiSelf, self).__init__()\n        self.posi = PositionalEncoding(dim1,dim2,dropout)\n        self.self_A = MultiSelfAttention(5,dim2)\n        self.self_B = MultiSelfAttention(5,dim1)\n        self.lstm = nn.LSTM(input_size=dim2,hidden_size=dim2,batch_first=True,bidirectional=False)\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.conv2 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        \n        x1 = self.conv1(x)\n        x1 = self.conv2(x1)  \n        x2 = self.posi(x)\n        x2,(h_n,c_n) = self.lstm(x2)\n        if x2.shape[1] == 1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n            x2,attn = self.self_B(x2,x2,x2)\n        else:\n            x2,attn = self.self_A(x2,x2,x2)\n        if x2.shape[2]==1:\n            x2 = x2.view(x2.shape[0],x2.shape[2],x2.shape[1])\n        out = x1+x2\n        #out = x2\n        out = self.bm1(out)\n        #out = out.view(out.shape[0],-1)\n        return out,x2\n    \nclass ModelB_MultiSelf_Pro(nn.Module):\n    def __init__(self, dim1, dim2, dropout=0.1):\n        super(ModelB_MultiSelf_Pro, self).__init__()\n        self.BS1 = ModelB_MultiSelf(dim1,dim2)\n        self.BS2 = ModelB_MultiSelf(dim1,dim2)\n\n        self.conv1 = nn.Conv1d(dim1,dim1,kernel_size=3,padding=1)\n        self.fn1 = nn.Linear(dim1*dim2,128)\n        self.fn2 = nn.Linear(128,1)\n        self.fn3 = nn.Linear(dim1*dim2,128)\n        self.fn4 = nn.Linear(128,1)\n        self.ac = nn.Sigmoid()\n        self.bm1 = nn.BatchNorm1d(dim1)\n\n    def forward(self, x):\n        x1,x2 = self.BS1(x)\n        x1,x2 = self.BS2(x1)\n\n        x1 = self.bm1(x1)\n        x1 = self.conv1(x1)\n        x1 = x1.contiguous().view(x1.shape[0],-1)\n        x1 = self.fn1(x1)\n        x1 = self.fn2(x1)\n        \n        x2 = x2.contiguous().view(x2.shape[0],-1)\n        x2 = self.fn3(x2)\n        x2 = self.fn4(x2)\n        \n        #out = x1\n        out = x1+x2\n        \n        out = self.ac(out)\n        return out\n    \ndef train(model,data,label,epoch,train_device,model_dir,batch_size):\n    if os.path.exists(model_dir+'model.pt'):\n        model_train = torch.load(model_dir+'/model.pt')\n    else:\n        model_train = model\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model_train.parameters(),lr=0.0001)\n    #optimizer = torch.optim.Adam(model_train.parameters(),lr=learn_rate)\n    #scheduler = StepLR(optimizer,step_size=10,gamma=0.5)\n    dataX = torch.Tensor(data).clone().detach()\n    label = torch.Tensor(label).clone().detach()\n    train_data = TensorDataset(dataX, label)\n    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n    running_loss = 0.0\n    model_train = model_train.to(train_device)\n    for batch_idx,data in enumerate(train_loader,0):\n        inputs,target = data\n        #inputs = inputs.reshape(inputs.shape[0], 1, inputs.shape[1])\n        inputs = inputs.to(train_device)\n        target = target.to(train_device)\n        target = target.reshape(target.shape[0],1)\n        optimizer.zero_grad()\n        outputs = model_train(inputs)\n        loss = criterion(outputs,target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if batch_idx == len(dataX)//batch_size:\n            #print('[%d, %5d] epoch loss: %.3f' %(epoch+1,batch_idx+1,running_loss))\n            print(running_loss)\n    save_model(model_train,model_dir)\n    model_train = torch.load(model_dir+'/model.pt')\n    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n    th,_,_,_,_,_,_,_,_,_ = metricsCal.evaluate(model_train,train_loader,train_device)\n    return running_loss,th\n\ndef test(data,label,best_auc,test_device,model_dir,batch_size,th):\n    model_test = load_model(model_dir)\n    #model_test.eval()\n    #model_test.to(test_device)\n    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n    test_data = TensorDataset(data,label)\n    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n    \n    print('Accuracy on test set: %d' %Acc)\n    print('Sensitivity on test set: %d' %Sen)\n    print('Speciality on test set: %d' %Spe)\n    print('MCC on test set: %.3f' %mcc)\n    print('auc on test set: %.3f' %AUC)\n    if(AUC > best_auc):\n        torch.save(model_test,model_dir+'model_best.pt')\n    return Acc, mcc, AUC\n\ndef independTest(data,label,test_device,model_dir,batch_size,th):\n    model_test = load_bestModel(model_dir)\n    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n    test_data = TensorDataset(data,label)\n    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n    _,_,_,_,Sen,Spe, Acc, mcc, AUC = metricsCal.evaluate(model_test,test_loader,test_device,False,th)\n    print(Acc,mcc,AUC)\n    print('Accuracy on test set: %d %%' %Acc)\n    print('Sensitivity on test set: %d %%' %Sen)\n    print('Speciality on test set: %d %%' %Spe)\n    print('MCC on test set: %.3f' %mcc)\n    print('auc on test set: %.3f' %AUC)\n    return Acc, mcc, AUC\n\ndef independResult(data,label,test_device,model_dir,batch_size,th):\n    model_test = load_bestModel(model_dir)\n    data = torch.Tensor(data).clone().detach()#torch.Tensor(data)\n    label = torch.Tensor(label).clone().detach()#.requires_grad_(True)torch.Tensor(label)\n    test_data = TensorDataset(data,label)\n    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n    #device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n    y_score,y_true = metricsCal.evaluate_result(model_test,test_loader,test_device,False,th)\n    return y_score,y_true\n\ndef analysis_results(pred,label,stack_data,stack_label,th=0.3,strategy=\"stack\"):\n    if strategy == \"soft\":\n        clf = SVC(kernel = \"rbf\",gamma=\"auto\", degree = 1,tol =1e-2, cache_size=7000)\n        clf.fit( stack_data, stack_label)\n        y_score = clf.predict_proba(pred)\n        th,_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics( clf.predict_proba(stack_data),stack_label )\n        TN, FN, FP, TP, Sen, Spe, Acc, mcc, AUC = metricsCal.get_test_metrics(y_score,label,th)\n    elif strategy == \"stack\":\n        y_score = np.mean(pred,axis=1)\n        th,_,_,_,_,_,_,_,_,_ = metricsCal.get_train_metrics( y_score,label )\n        TN, FN, FP, TP, Sen, Spe, Acc, mcc, AUC = metricsCal.get_test_metrics(y_score,label,th)\n    return y_score,TN, FN, FP, TP, Sen, Spe, Acc, mcc, AUC\n\ndef load_model(model_dir):\n    if os.path.exists(model_dir+'model.pt'):\n        model_load = torch.load(model_dir+'model.pt')\n    return model_load\n\ndef save_model(model_save,model_dir):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    torch.save(model_save, model_dir+'model.pt')\n\ndef load_bestModel(model_dir):\n    if os.path.exists(model_dir+'model_best.pt'):\n        model_load = torch.load(model_dir+'model_best.pt',map_location='cuda:0')\n    return model_load","metadata":{"execution":{"iopub.status.busy":"2024-06-30T17:37:50.247645Z","iopub.execute_input":"2024-06-30T17:37:50.247976Z","iopub.status.idle":"2024-06-30T17:37:50.357105Z","shell.execute_reply.started":"2024-06-30T17:37:50.247950Z","shell.execute_reply":"2024-06-30T17:37:50.356211Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# from features import ensembleFeature\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport torch\nfrom torch import nn\n# from utils import metricsCal\nfrom torch.utils.data import DataLoader, TensorDataset\nimport math\nimport sys\nimport copy\nimport pickle\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport os\n# import Attention_model as Model\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\nimport csv\n\ndef read_fasta(fasta_file_path):\n    sequences = []\n    names = []\n    with open(fasta_file_path, 'r') as fasta_file:\n        for line in fasta_file:\n            if line.startswith('>'):\n                name = line.strip()[1:]\n                names.append(name)\n                sequence = ''\n            else:\n                sequence += line.strip()\n                sequences.append(sequence)\n    return sequences, names\n\nif __name__ == '__main__':\n    # Set device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Hyper Parameters\n    max_epochs = 50\n    max_patience = 10\n    batch_size = 32\n    cuda_device = 0\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_device)\n    \n    # Load training data\n    train_seq, train_name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_train.fasta\")\n    trainPos_seq = [seq for i, seq in enumerate(train_seq) if train_name[i][0] == 'P']\n    trainNeg_seq = [seq for i, seq in enumerate(train_seq) if train_name[i][0] != 'P']\n\n    # Load testing data\n    test_seq, test_name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_ind.fasta\")\n    testPos_seq = [seq for i, seq in enumerate(test_seq) if test_name[i][0] == 'P']\n    testNeg_seq = [seq for i, seq in enumerate(test_seq) if test_name[i][0] != 'P']\n\n    # Generate features\n    print(\"The model is generating the w2v feature from the seq now!\")\n    trainPos_emb = ensembleFeature.emb_seqs(trainPos_seq)\n    trainNeg_emb = ensembleFeature.emb_seqs(trainNeg_seq)\n    testPos_emb = ensembleFeature.emb_seqs(testPos_seq)\n    testNeg_emb = ensembleFeature.emb_seqs(testNeg_seq)\n\n    print(\"The model is generating the PSNP feature from the seq now!\")\n    trainPos_PSNP, trainNeg_PSNP, testPos_PSNP, testNeg_PSNP = ensembleFeature.PSNP(trainPos_seq, trainNeg_seq, testPos_seq, testNeg_seq)\n\n    print(\"The model is generating the PCP feature from the seq now, please wait...\")\n    trainPos_PCP = ensembleFeature.PCP(trainPos_seq)\n    trainNeg_PCP = ensembleFeature.PCP(trainNeg_seq)\n    testPos_PCP = ensembleFeature.PCP(testPos_seq)\n    testNeg_PCP = ensembleFeature.PCP(testNeg_seq)\n\n    print(\"The model is generating the DBPF feature from the seq now!\")\n    trainPos_DBPF = ensembleFeature.DBPF(trainPos_seq)\n    trainNeg_DBPF = ensembleFeature.DBPF(trainNeg_seq)\n    testPos_DBPF = ensembleFeature.DBPF(testPos_seq)\n    testNeg_DBPF = ensembleFeature.DBPF(testNeg_seq)\n\n    # Reshape features\n    trainPos_PSNP = trainPos_PSNP.reshape(trainPos_PSNP.shape[0], 1, trainPos_PSNP.shape[1])\n    trainNeg_PSNP = trainNeg_PSNP.reshape(trainNeg_PSNP.shape[0], 1, trainNeg_PSNP.shape[1])\n    testPos_PSNP = testPos_PSNP.reshape(testPos_PSNP.shape[0], 1, testPos_PSNP.shape[1])\n    testNeg_PSNP = testNeg_PSNP.reshape(testNeg_PSNP.shape[0], 1, testNeg_PSNP.shape[1])\n\n    trainPos_PCP = trainPos_PCP.reshape(trainPos_PCP.shape[0], 1, trainPos_PCP.shape[1])\n    trainNeg_PCP = trainNeg_PCP.reshape(trainNeg_PCP.shape[0], 1, trainNeg_PCP.shape[1])\n    testPos_PCP = testPos_PCP.reshape(testPos_PCP.shape[0], 1, testPos_PCP.shape[1])\n    testNeg_PCP = testNeg_PCP.reshape(testNeg_PCP.shape[0], 1, testNeg_PCP.shape[1])\n\n    # K-Fold Cross-Validation\n    kf = KFold(5, shuffle=True)\n    for i, (train_index, val_index) in enumerate(kf.split(trainNeg_emb)):\n        X_train_PCP_neg = trainNeg_PCP[train_index]\n        X_val_PCP_neg = trainNeg_PCP[val_index]\n        X_train_PCP_pos = trainPos_PCP\n        X_val_PCP_pos = trainPos_PCP\n        Y_train_PCP = np.append(np.ones(len(X_train_PCP_pos)), np.zeros(len(X_train_PCP_neg)), axis=0)\n        Y_val_PCP = np.append(np.ones(len(X_val_PCP_pos)), np.zeros(len(X_val_PCP_neg)), axis=0)\n        X_train_PCP = np.append(X_train_PCP_pos, X_train_PCP_neg, axis=0)\n        X_val_PCP = np.append(X_val_PCP_pos, X_val_PCP_neg, axis=0)\n\n        X_train_emb_neg = trainNeg_emb[train_index]\n        X_val_emb_neg = trainNeg_emb[val_index]\n        X_train_emb_pos = trainPos_emb\n        X_val_emb_pos = trainPos_emb\n        Y_train_emb = np.append(np.ones(len(X_train_emb_pos)), np.zeros(len(X_train_emb_neg)), axis=0)\n        Y_val_emb = np.append(np.ones(len(X_val_emb_pos)), np.zeros(len(X_val_emb_neg)), axis=0)\n        X_train_emb = np.append(X_train_emb_pos, X_train_emb_neg, axis=0)\n        X_val_emb = np.append(X_val_emb_pos, X_val_emb_neg, axis=0)\n\n        X_train_PSNP_neg = trainNeg_PSNP[train_index]\n        X_val_PSNP_neg = trainNeg_PSNP[val_index]\n        X_train_PSNP_pos = trainPos_PSNP\n        X_val_PSNP_pos = trainPos_PSNP\n        Y_train_PSNP = np.append(np.ones(len(X_train_PSNP_pos)), np.zeros(len(X_train_PSNP_neg)), axis=0)\n        Y_val_PSNP = np.append(np.ones(len(X_val_PSNP_pos)), np.zeros(len(X_val_PSNP_neg)), axis=0)\n        X_train_PSNP = np.append(X_train_PSNP_pos, X_train_PSNP_neg, axis=0)\n        X_val_PSNP = np.append(X_val_PSNP_pos, X_val_PSNP_neg, axis=0)\n\n        X_train_DBPF_neg = trainNeg_DBPF[train_index]\n        X_val_DBPF_neg = trainNeg_DBPF[val_index]\n        X_train_DBPF_pos = trainPos_DBPF\n        X_val_DBPF_pos = trainPos_DBPF\n        Y_train_DBPF = np.append(np.ones(len(X_train_DBPF_pos)), np.zeros(len(X_train_DBPF_neg)), axis=0)\n        Y_val_DBPF = np.append(np.ones(len(X_val_DBPF_pos)), np.zeros(len(X_val_DBPF_neg)), axis=0)\n        X_train_DBPF = np.append(X_train_DBPF_pos, X_train_DBPF_neg, axis=0)\n        X_val_DBPF = np.append(X_val_DBPF_pos, X_val_DBPF_neg, axis=0)\n\n        # Prepare DataLoaders\n        trainData_emb = TensorDataset(torch.tensor(X_train_emb).float().to(device), torch.tensor(Y_train_emb).float().to(device))\n        valData_emb = TensorDataset(torch.tensor(X_val_emb).float().to(device), torch.tensor(Y_val_emb).float().to(device))\n        trainDataLoader_emb = DataLoader(trainData_emb, batch_size=batch_size, shuffle=True)\n        valDataLoader_emb = DataLoader(valData_emb, batch_size=batch_size, shuffle=True)\n\n        trainData_PCP = TensorDataset(torch.tensor(X_train_PCP).float().to(device), torch.tensor(Y_train_PCP).float().to(device))\n        valData_PCP = TensorDataset(torch.tensor(X_val_PCP).float().to(device), torch.tensor(Y_val_PCP).float().to(device))\n        trainDataLoader_PCP = DataLoader(trainData_PCP, batch_size=batch_size, shuffle=True)\n        valDataLoader_PCP = DataLoader(valData_PCP, batch_size=batch_size, shuffle=True)\n\n        trainData_PSNP = TensorDataset(torch.tensor(X_train_PSNP).float().to(device), torch.tensor(Y_train_PSNP).float().to(device))\n        valData_PSNP = TensorDataset(torch.tensor(X_val_PSNP).float().to(device), torch.tensor(Y_val_PSNP).float().to(device))\n        trainDataLoader_PSNP = DataLoader(trainData_PSNP, batch_size=batch_size, shuffle=True)\n        valDataLoader_PSNP = DataLoader(valData_PSNP, batch_size=batch_size, shuffle=True)\n\n        trainData_DBPF = TensorDataset(torch.tensor(X_train_DBPF).float().to(device), torch.tensor(Y_train_DBPF).float().to(device))\n        valData_DBPF = TensorDataset(torch.tensor(X_val_DBPF).float().to(device), torch.tensor(Y_val_DBPF).float().to(device))\n        trainDataLoader_DBPF = DataLoader(trainData_DBPF, batch_size=batch_size, shuffle=True)\n        valDataLoader_DBPF = DataLoader(valData_DBPF, batch_size=batch_size, shuffle=True)\n\n        # Initialize models\n        models = {\n            \"emb\": Model.AttentionNet().to(device),\n            \"PCP\": Model.AttentionNet().to(device),\n            \"PSNP\": Model.AttentionNet().to(device),\n            \"DBPF\": Model.AttentionNet().to(device)\n        }\n\n        optimizers = {\n            \"emb\": torch.optim.Adam(models[\"emb\"].parameters(), lr=0.0005),\n            \"PCP\": torch.optim.Adam(models[\"PCP\"].parameters(), lr=0.0005),\n            \"PSNP\": torch.optim.Adam(models[\"PSNP\"].parameters(), lr=0.0005),\n            \"DBPF\": torch.optim.Adam(models[\"DBPF\"].parameters(), lr=0.0005)\n        }\n\n        best_auc = {\"emb\": 0, \"PCP\": 0, \"PSNP\": 0, \"DBPF\": 0}\n        patience_counter = {\"emb\": 0, \"PCP\": 0, \"PSNP\": 0, \"DBPF\": 0}\n\n        # Training and validation\n        for epoch in range(max_epochs):\n            for feature in models.keys():\n                model = models[feature]\n                optimizer = optimizers[feature]\n\n                model.train()\n                train_loss = 0.0\n                for inputs, labels in eval(f\"trainDataLoader_{feature}\"):\n                    inputs, labels = inputs.to(device), labels.to(device)\n\n                    optimizer.zero_grad()\n                    outputs = model(inputs)\n                    loss = torch.nn.BCELoss()(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n\n                    train_loss += loss.item() * inputs.size(0)\n\n                model.eval()\n                val_loss = 0.0\n                val_outputs = []\n                val_labels = []\n                with torch.no_grad():\n                    for inputs, labels in eval(f\"valDataLoader_{feature}\"):\n                        inputs, labels = inputs.to(device), labels.to(device)\n\n                        outputs = model(inputs)\n                        loss = torch.nn.BCELoss()(outputs, labels)\n\n                        val_loss += loss.item() * inputs.size(0)\n                        val_outputs.extend(outputs.cpu().numpy())\n                        val_labels.extend(labels.cpu().numpy())\n\n                # Calculate validation AUC\n                val_outputs = np.array(val_outputs)\n                val_labels = np.array(val_labels)\n                val_auc = roc_auc_score(val_labels, val_outputs)\n\n                if val_auc > best_auc[feature]:\n                    best_auc[feature] = val_auc\n                    patience_counter[feature] = 0\n                else:\n                    patience_counter[feature] += 1\n\n                if patience_counter[feature] >= max_patience:\n                    print(f\"Early stopping for feature {feature} at epoch {epoch+1}\")\n                    break\n\n    print(\"Training complete.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from features import ensembleFeature\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport torch\nfrom torch import nn\nimport re\nfrom sklearn.utils import shuffle\n# from utils import metricsCal\nfrom torch.utils.data import DataLoader, TensorDataset\nimport math\nimport sys\nimport copy\nimport pickle\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport os\n# import Attention_model as Model\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\nimport csv\n\n\ndef read_fasta(fasta_file_path):\n\n    sequences = []\n    names = []\n\n    with open(fasta_file_path, 'r') as fasta_file:\n        for line in fasta_file:\n            if line.startswith('>'):\n                name = line.strip()[1:]\n                names.append(name)\n                sequence = ''\n            else:\n                sequence += line.strip()\n                sequences.append(sequence)\n    return sequences, names\n\n\n# Change the length of seq from 1001 to 41.\ndef long_short(data):\n    seq_list = []\n    for i in data[0]:\n        seq_list.append(str(i)[480:521])\n    return np.array(seq_list)\n\n\n# Delete the seq if its bases contain 'N', which is not 'ATCG'\ndef check_N(data1):\n    seq_list = []\n    for i in range(len(data1)):\n        if str(data1[i]).find(\"N\") < 0:\n            seq_list.append(data1[i])\n    return np.array(seq_list)\n\n\nif __name__ == '__main__':\n\n    device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n    device_cpu = torch.device(\"cpu\")\n\n    # Directly set the parameters\n#     type_name = \"YourType\"  # Replace with your type name\n#     cell_name = \"YourCell\"  # Replace with your cell name\n\n    # Hyper Parameters\n    max_epochs = 50\n    max_patience = 10\n    batch_size = 32\n    cuda_device = 0\n\n    # Choose the device\n    os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_device)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    train_seq, name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_train.fasta\")\n\n    trainPos_seq = []\n    trainNeg_seq = []\n    for i in range(len(name)):\n        if name[i][0] == 'P':\n            trainPos_seq.append(train_seq[i])\n        else:\n            trainNeg_seq.append(train_seq[i])\n\n    test_seq, name = read_fasta(\"/kaggle/input/5mcsite-dnn/Arabidopsis_ind.fasta\")\n\n    testPos_seq = []\n    testNeg_seq = []\n    for i in range(len(name)):\n        if name[i][0] == 'P':\n            testPos_seq.append(test_seq[i])\n        else:\n            testNeg_seq.append(test_seq[i])\n\n    # Shuffle the seq\n    trainNeg_seq = shuffle(trainNeg_seq, random_state=1)\n    testNeg_seq = shuffle(testNeg_seq, random_state=1)\n\n    # Choose the seq\n    trainNeg_seq = trainNeg_seq[:len(trainPos_seq) * 10]\n    testNeg_seq = testNeg_seq[:len(testPos_seq)]\n\n    # Generate the feature\n    print(\"The model is generating the w2v feature from the seq now!\")\n    trainPos_emb = emb_seqs(trainPos_seq)\n    trainNeg_emb = emb_seqs(trainNeg_seq)\n\n    print(\"The model is generating the PSNP feature from the seq now!\")\n    trainPos_PSNP, trainNeg_PSNP, testPos_PSNP, testNeg_PSNP = PSNP(trainPos_seq, trainNeg_seq,\n                                                                                    testPos_seq, testNeg_seq)\n\n    print(\"The model is generating the PCP feature from the seq now, please wait this will take some time\")\n    trainPos_PCP = PCP(trainPos_seq)\n    trainNeg_PCP = PCP(trainNeg_seq)\n\n    print(\"The model is generating the DBPF feature from the seq now!\")\n    trainPos_DBPF = DBPF(trainPos_seq)\n    trainNeg_DBPF = DBPF(trainNeg_seq)\n    \n   # Flatten the arrays to 2D\n    trainPos_DBPF_flat = trainPos_DBPF.reshape(trainPos_DBPF.shape[0], -1)\n    trainNeg_DBPF_flat = trainNeg_DBPF.reshape(trainNeg_DBPF.shape[0], -1)\n\n    trainPos_emb_flat = trainPos_emb.reshape(trainPos_emb.shape[0], -1)\n    trainNeg_emb_flat = trainNeg_emb.reshape(trainNeg_emb.shape[0], -1)\n\n    # Determine the maximum number of columns among all feature arrays\n    max_columns = max(\n        trainPos_PCP.shape[1],\n        trainPos_PSNP.shape[1],\n        trainPos_DBPF_flat.shape[1],\n        trainPos_emb_flat.shape[1]\n    )\n\n\n    trainPos_PSNP = trainPos_PSNP.reshape(trainPos_PSNP.shape[0], 1, trainPos_PSNP.shape[1])\n    trainNeg_PSNP = trainNeg_PSNP.reshape(trainNeg_PSNP.shape[0], 1, trainNeg_PSNP.shape[1])\n\n    trainPos_PCP = trainPos_PCP.reshape(trainPos_PCP.shape[0], 1, trainPos_PCP.shape[1])\n    trainNeg_PCP = trainNeg_PCP.reshape(trainNeg_PCP.shape[0], 1, trainNeg_PCP.shape[1])\n    \n    # Print the shapes after flattening\n    print(f\"Shape of trainPos_DBPF_flat: {trainPos_DBPF_flat.shape}\")\n    print(f\"Shape of trainNeg_DBPF_flat: {trainNeg_DBPF_flat.shape}\")\n    print(f\"Shape of trainPos_emb_flat: {trainPos_emb_flat.shape}\")\n    print(f\"Shape of trainNeg_emb_flat: {trainNeg_emb_flat.shape}\")\n\n    # Do the 5-fold cross-validation\n    kf = KFold(5, shuffle=True)\n    for i, (train_index, test_index) in enumerate(kf.split(trainNeg_emb_flat)):\n        # Generate the data for training and validation\n        X_train_PCP_neg = trainNeg_PCP[train_index]\n        X_test_PCP_neg = trainNeg_PCP[test_index]\n        trainPos_PCP_rep = np.repeat(trainPos_PCP, 10, axis=0)\n        X_train_PCP_pos = trainPos_PCP_rep[:int(0.8 * len(trainPos_PCP_rep))]\n        X_test_PCP_pos = trainPos_PCP_rep[int(0.8 * len(trainPos_PCP_rep)):]\n        Y_train_PCP = np.append(np.ones(len(X_train_PCP_pos)), np.zeros(len(X_train_PCP_neg)), axis=0)\n        Y_test_PCP = np.append(np.ones(len(X_test_PCP_pos)), np.zeros(len(X_test_PCP_neg)), axis=0)\n        X_train_PCP = np.append(X_train_PCP_pos, X_train_PCP_neg, axis=0)\n        X_test_PCP = np.append(X_test_PCP_pos, X_test_PCP_neg, axis=0)\n        X_train_PCP, Y_train_PCP = shuffle(X_train_PCP, Y_train_PCP, random_state=42)\n        X_test_PCP, Y_test_PCP = shuffle(X_test_PCP, Y_test_PCP, random_state=42)\n\n        X_train_emb_neg = trainNeg_emb_flat[train_index]\n        X_test_emb_neg = trainNeg_emb_flat[test_index]\n        trainPos_emb_rep = np.repeat(trainPos_emb_flat, 10, axis=0)\n        X_train_emb_pos = trainPos_emb_rep[:int(0.8 * len(trainPos_emb_rep))]\n        X_test_emb_pos = trainPos_emb_rep[int(0.8 * len(trainPos_emb_rep)):]\n        Y_train_emb = np.append(np.ones(len(X_train_emb_pos)), np.zeros(len(X_train_emb_neg)), axis=0)\n        Y_test_emb = np.append(np.ones(len(X_test_emb_pos)), np.zeros(len(X_test_emb_neg)), axis=0)\n        X_train_emb = np.append(X_train_emb_pos, X_train_emb_neg, axis=0)\n        X_test_emb = np.append(X_test_emb_pos, X_test_emb_neg, axis=0)\n        X_train_emb, Y_train_emb = shuffle(X_train_emb, Y_train_emb, random_state=42)\n        X_test_emb, Y_test_emb = shuffle(X_test_emb, Y_test_emb, random_state=42)\n\n        X_train_DBPF_neg = trainNeg_DBPF_flat[train_index]\n        X_test_DBPF_neg = trainNeg_DBPF_flat[test_index]\n        trainPos_DBPF_rep = np.repeat(trainPos_DBPF_flat, 10, axis=0)\n        X_train_DBPF_pos = trainPos_DBPF_rep[:int(0.8 * len(trainPos_DBPF_rep))]\n        X_test_DBPF_pos = trainPos_DBPF_rep[int(0.8 * len(trainPos_DBPF_rep)):]\n        Y_train_DBPF = np.append(np.ones(len(X_train_DBPF_pos)), np.zeros(len(X_train_DBPF_neg)), axis=0)\n        Y_test_DBPF = np.append(np.ones(len(X_test_DBPF_pos)), np.zeros(len(X_test_DBPF_neg)), axis=0)\n        X_train_DBPF = np.append(X_train_DBPF_pos, X_train_DBPF_neg, axis=0)\n        X_test_DBPF = np.append(X_test_DBPF_pos, X_test_DBPF_neg, axis=0)\n        X_train_DBPF, Y_train_DBPF = shuffle(X_train_DBPF, Y_train_DBPF, random_state=42)\n        X_test_DBPF, Y_test_DBPF = shuffle(X_test_DBPF, Y_test_DBPF, random_state=42)\n\n        X_train_PSNP_neg = trainNeg_PSNP[train_index]\n        X_test_PSNP_neg = trainNeg_PSNP[test_index]\n        trainPos_PSNP_rep = np.repeat(trainPos_PSNP, 10, axis=0)\n        X_train_PSNP_pos = trainPos_PSNP_rep[:int(0.8 * len(trainPos_PSNP_rep))]\n        X_test_PSNP_pos = trainPos_PSNP_rep[int(0.8 * len(trainPos_PSNP_rep)):]\n        Y_train_PSNP = np.append(np.ones(len(X_train_PSNP_pos)), np.zeros(len(X_train_PSNP_neg)), axis=0)\n        Y_test_PSNP = np.append(np.ones(len(X_test_PSNP_pos)), np.zeros(len(X_test_PSNP_neg)), axis=0)\n        X_train_PSNP = np.append(X_train_PSNP_pos, X_train_PSNP_neg, axis=0)\n        X_test_PSNP = np.append(X_test_PSNP_pos, X_test_PSNP_neg, axis=0)\n        X_train_PSNP, Y_train_PSNP = shuffle(X_train_PSNP, Y_train_PSNP, random_state=42)\n        X_test_PSNP, Y_test_PSNP = shuffle(X_test_PSNP, Y_test_PSNP, random_state=42)\n\n        # Print shapes before concatenation\n        print(f\"X_train_PCP shape: {X_train_PCP.shape}\")\n        print(f\"X_train_PSNP shape: {X_train_PSNP.shape}\")\n        print(f\"X_train_DBPF_flat shape: {X_train_DBPF.shape}\")\n        print(f\"X_train_emb_flat shape: {X_train_emb.shape}\")\n\n        # Concatenate features\n        X_train = np.concatenate([X_train_PCP, X_train_PSNP, X_train_DBPF, X_train_emb], axis=1)\n        X_test = np.concatenate([X_test_PCP, X_test_PSNP, X_test_DBPF, X_test_emb], axis=1)\n\n        print(f\"X_train shape after concatenation: {X_train.shape}\")\n        print(f\"X_test shape after concatenation: {X_test.shape}\")\n\n        x_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n        x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n        y_train_tensor = torch.tensor(Y_train_PCP, dtype=torch.long)\n        y_test_tensor = torch.tensor(Y_test_PCP, dtype=torch.long)\n\n        train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n        test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n        print(\"The model is building the RNN_AE model now!\")\n        model = Model.RNN_AE(input_size=x_train_tensor.shape[1])\n        model.to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)\n\n        best_val_acc = 0.0\n        patience_counter = 0\n        for epoch in range(max_epochs):\n            model.train()\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in train_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                _, preds = torch.max(outputs, 1)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(train_loader.dataset)\n            epoch_acc = running_corrects.double() / len(train_loader.dataset)\n\n            model.eval()\n            val_loss = 0.0\n            val_corrects = 0\n            with torch.no_grad():\n                for inputs, labels in test_loader:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    val_loss += loss.item() * inputs.size(0)\n                    _, preds = torch.max(outputs, 1)\n                    val_corrects += torch.sum(preds == labels.data)\n\n            val_loss /= len(test_loader.dataset)\n            val_acc = val_corrects.double() / len(test_loader.dataset)\n\n            print(\n                f\"Epoch {epoch + 1}/{max_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n            scheduler.step(val_loss)\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                patience_counter = 0\n                best_model_wts = copy.deepcopy(model.state_dict())\n            else:\n                patience_counter += 1\n\n            if patience_counter >= max_patience:\n                break\n\n        model.load_state_dict(best_model_wts)\n        model.eval()\n        test_corrects = 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                test_corrects += torch.sum(preds == labels.data)\n\n        test_acc = test_corrects.double() / len(test_loader.dataset)\n        print(f\"Test Accuracy: {test_acc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T17:20:19.423422Z","iopub.execute_input":"2024-06-30T17:20:19.423763Z","iopub.status.idle":"2024-06-30T17:25:57.359996Z","shell.execute_reply.started":"2024-06-30T17:20:19.423737Z","shell.execute_reply":"2024-06-30T17:25:57.358866Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The model is generating the w2v feature from the seq now!\nThe model is generating the PSNP feature from the seq now!\nThe model is generating the PCP feature from the seq now, please wait this will take some time\nThe model is generating the DBPF feature from the seq now!\nFlatten the  DBPF and emb feature from the seq now!\nShape of trainPos_DBPF_flat: (5289, 200)\nShape of trainNeg_DBPF_flat: (5289, 200)\nShape of trainPos_emb_flat: (5289, 3800)\nShape of trainNeg_emb_flat: (5289, 3800)\nX_train_PCP shape: (46543, 1, 400)\nX_train_PSNP shape: (46543, 1, 41)\nX_train_DBPF_flat shape: (46543, 200)\nX_train_emb_flat shape: (46543, 3800)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train_emb_flat shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Concatenate features\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_PCP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_PSNP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_DBPF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([X_test_PCP, X_test_PSNP, X_test_DBPF, X_test_emb], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape after concatenation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 400 and the array at index 1 has size 41"],"ename":"ValueError","evalue":"all the input array dimensions except for the concatenation axis must match exactly, but along dimension 2, the array at index 0 has size 400 and the array at index 1 has size 41","output_type":"error"}]}]}